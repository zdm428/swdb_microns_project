{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4d3f3a-c742-4c4d-a836-57382d055170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import caveclient\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac861fb9-ac20-4dfb-b21a-a8b1cee7a375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "platstring = platform.platform()\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2023/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on Code Ocean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2023/\"\n",
    "    \n",
    "data_dir = os.path.join(data_root, 'microns_in_silico')\n",
    "\n",
    "# you can just override this if the location of the data varies\n",
    "# data_dir = '/Users/forrestc/Downloads/microns_in_silico/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79ef42c-e6d2-4fb8-be6c-adcb68355c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we are going to load up the data and prepare the dataframe like we did \n",
    "# in class but with fewer comments\n",
    "\n",
    "# load up the in-silico responses as a pandas dataframe from a numpy array \n",
    "resp=pd.DataFrame(np.load(os.path.join(data_dir, 'nat_resp.npy')))\n",
    "\n",
    "# load up the csv of metadata about the 104171 units\n",
    "units_df = pd.read_csv(os.path.join(data_dir, 'nat_unit.csv'))\n",
    "\n",
    "# set the index to the be the row_idx of the units_df\n",
    "resp.index = units_df['row_idx']\n",
    "\n",
    "# if we are on code ocean, the CAVEsetup helped you make your token an environment variable\n",
    "if 'amzn' in platstring:\n",
    "    client= caveclient.CAVEclient('minnie65_public', auth_token=os.environ['API_SECRET'])\n",
    "else:\n",
    "    # otherwise if you are local, then it should be saved to a file in your harddrive \n",
    "    # that the caveclient knows where to read.\n",
    "    client= caveclient.CAVEclient('minnie65_public')\n",
    "\n",
    "# lets pull out the manual coregistered neurons\n",
    "# desired_resolution describes how many nanometers you want each unit to be\n",
    "# so 1000,1000,1000 gives positions in microns for x,y and z\n",
    "coreg_df = client.materialize.query_table('coregistration_manual_v3', desired_resolution=[1000,1000,1000])\n",
    "\n",
    "# lets merge these dataframes so we get the row_idx of each coregistered unit\n",
    "# we merge on the corresponding columns, however scan was called something\n",
    "# slightly different in one csv vs the CAVE table\n",
    "coreg_in_silico=pd.merge(units_df, coreg_df, \n",
    "         left_on=['scan_session', 'scan_idx', 'unit_id'],\n",
    "          right_on=['session','scan_idx', 'unit_id'])\n",
    "# reset the index to make sure that we have the index\n",
    "coreg_in_silico.reset_index(inplace=True)\n",
    "\n",
    "# this will pull out the responses to the coregistered units\n",
    "# by using the row_idx that was provided in the metadata\n",
    "coreg_resp = resp.loc[coreg_in_silico.row_idx,:]\n",
    "\n",
    "# now with a reduced set of units, we can calculate the Pearson correlation\n",
    "# between their responses\n",
    "corr_M = np.corrcoef(coreg_resp.values)\n",
    "\n",
    "ct_df = client.materialize.query_table('aibs_soma_nuc_exc_mtype_preds_v117')\n",
    "# lets merge it on the coregistered cells with in silico responses\n",
    "# we will use the segment version is (pt_root_id) to do this\n",
    "ct_merge_df=pd.merge(coreg_in_silico.reset_index(),\n",
    "                     ct_df[['pt_root_id', 'id_ref', 'cell_type']],\n",
    "                     on='pt_root_id')\n",
    "\n",
    "# lets pull all the neurons where we can trust the axons\n",
    "# as being reasonably well reconstructed\n",
    "prf_df=client.materialize.query_table('proofreading_status_public_release', \n",
    "                                      filter_in_dict={'status_axon': ['extended', 'clean']})\n",
    "\n",
    "# how many of these are coregistered?\n",
    "clean_coreg_df = pd.merge(prf_df, coreg_in_silico, on='pt_root_id')\n",
    "\n",
    "# we need this code to work in solutions directory\n",
    "# and one up..\n",
    "# if 'solutions' in os.getcwd():\n",
    "#     workshop2file = '../../workshop2/all_prf_coreg_conn_v661.pkl'\n",
    "# else:\n",
    "#     workshop2file = '../workshop2/all_prf_coreg_conn_v661.pkl'\n",
    "workshop2file = 'all_prf_coreg_conn_v661.pkl'\n",
    "all_syn_df = pd.read_pickle(workshop2file)\n",
    "\n",
    "nuc_df = client.materialize.query_view('nucleus_detection_lookup_v1', \n",
    "                                        select_columns = ['id', 'pt_root_id', 'pt_position'],\n",
    "                                        desired_resolution=[1000,1000,1000])\n",
    "\n",
    "# lets merge on the pre and post-synaptic positions of these connections\n",
    "\n",
    "# renaming the positions as pre and post depending on how we did the merge\n",
    "# and drop the duplicate id columns\n",
    "all_syn_dfm=all_syn_df.merge(nuc_df[['id', 'pt_position']], left_on='pre_nuc_id', right_on='id')\\\n",
    ".rename({'pt_position':'pre_pt_position'}, axis=1)\\\n",
    ".merge(nuc_df[['id', 'pt_position']], left_on='post_nuc_id', right_on='id')\\\n",
    ".rename({'pt_position':'post_pt_position'}, axis=1)\\\n",
    ".drop(['id_x', 'id_y'], axis=1)\n",
    "\n",
    "# now lets merge in the neurons that are coregistered with responses\n",
    "\n",
    "# we have to drop duplicates to avoid the few cells that were coregistered twice \n",
    "# being double counted\n",
    "all_syn_dfm2=all_syn_dfm.merge(coreg_in_silico[['index','target_id', 'scan_session', 'scan_idx', 'field','unit_id', 'score', 'residual']],\n",
    "                  left_on='pre_nuc_id', \n",
    "                  right_on='target_id')\\\n",
    ".merge(coreg_in_silico[['index','target_id', 'scan_session', 'scan_idx', 'field','unit_id','score', 'residual']],\n",
    "                  left_on='post_nuc_id', \n",
    "                  right_on='target_id',\n",
    "                  suffixes=['_pre', '_post'])\\\n",
    ".drop(['target_id_pre', 'target_id_post'],axis=1)\\\n",
    ".drop_duplicates(subset=['pre_nuc_id', 'post_nuc_id'])\n",
    "all_syn_dfm2\n",
    "\n",
    "# now use fancy indexing to pull out the correlation associated with each of these connections\n",
    "all_syn_dfm2['C']=corr_M[all_syn_dfm2.index_pre, all_syn_dfm2.index_post]\n",
    "\n",
    "\n",
    "# now lets merge in our cell type calls\n",
    "# by using suffixes we will name the pre and post synaptic cell type \n",
    "# differently\n",
    "all_syn_dfm3=all_syn_dfm2.merge(ct_df[['target_id', 'cell_type']],\n",
    "                  left_on='pre_nuc_id', \n",
    "                  right_on='target_id')\\\n",
    ".merge(ct_df[['target_id', 'cell_type']],\n",
    "                  left_on='post_nuc_id', \n",
    "                  right_on='target_id',\n",
    "                  suffixes=['_pre', '_post'])\\\n",
    ".drop(['target_id_pre', 'target_id_post'],axis=1)\\\n",
    ".drop_duplicates(subset=['pre_nuc_id', 'post_nuc_id'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3006a57-6124-4758-9b17-86089496825b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_pt_root_id</th>\n",
       "      <th>post_pt_root_id</th>\n",
       "      <th>n_syn</th>\n",
       "      <th>sum_size</th>\n",
       "      <th>C</th>\n",
       "      <th>cell_type_pre</th>\n",
       "      <th>cell_type_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691135927049742</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>1732</td>\n",
       "      <td>0.169119</td>\n",
       "      <td>L5ET</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864691136228183377</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>7604</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>L4c</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864691135155894884</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>9404</td>\n",
       "      <td>0.040026</td>\n",
       "      <td>L2a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>864691135591041291</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>2</td>\n",
       "      <td>25900</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>L2a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864691136194822888</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>9972</td>\n",
       "      <td>0.191115</td>\n",
       "      <td>L4a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pre_pt_root_id     post_pt_root_id  n_syn  sum_size         C  \\\n",
       "0  864691135927049742  864691136310417242      1      1732  0.169119   \n",
       "1  864691136228183377  864691136310417242      1      7604  0.010837   \n",
       "2  864691135155894884  864691136310417242      1      9404  0.040026   \n",
       "3  864691135591041291  864691136310417242      2     25900  0.145264   \n",
       "4  864691136194822888  864691136310417242      1      9972  0.191115   \n",
       "\n",
       "  cell_type_pre cell_type_post  \n",
       "0          L5ET           L5ET  \n",
       "1           L4c           L5ET  \n",
       "2           L2a           L5ET  \n",
       "3           L2a           L5ET  \n",
       "4           L4a           L5ET  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check we got the same thing\n",
    "all_syn_dfm3[['pre_pt_root_id', 'post_pt_root_id', 'n_syn',\n",
    "              'sum_size', 'C', 'cell_type_pre', 'cell_type_post']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b71eb0-133b-4668-a82e-059b91f8656d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864691136521831825    293\n",
       "864691135473091634    218\n",
       "864691136084747372    216\n",
       "864691135701034107    205\n",
       "864691135975539779    197\n",
       "864691136108938168    186\n",
       "864691135915343462    186\n",
       "864691134941194851    172\n",
       "864691136923990500    169\n",
       "864691136023889209    167\n",
       "Name: pre_pt_root_id, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the connections by pre synaptic cell\n",
    "counts=all_syn_dfm3.pre_pt_root_id.value_counts()\n",
    "# look at the top 10\n",
    "counts.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261a9bd8-7bdd-4b02-be8c-97fe75631b25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find the average correlation and standard deviations of correlations for each individual cell\n",
    "# hint: use group by\n",
    "avgC=all_syn_dfm3.groupby('pre_pt_root_id').C.mean()\n",
    "avgC.name= 'meanC'\n",
    "stdC = all_syn_dfm3.groupby('pre_pt_root_id').C.std()\n",
    "stdC.name = 'stdC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081ac6df-a0a4-46f4-9b47-af568aad5762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_pt_root_id</th>\n",
       "      <th>meanC</th>\n",
       "      <th>stdC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864691136521831825</th>\n",
       "      <td>293.0</td>\n",
       "      <td>0.069778</td>\n",
       "      <td>0.143706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691135473091634</th>\n",
       "      <td>218.0</td>\n",
       "      <td>0.013846</td>\n",
       "      <td>0.114538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691136084747372</th>\n",
       "      <td>216.0</td>\n",
       "      <td>0.174026</td>\n",
       "      <td>0.161509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691135701034107</th>\n",
       "      <td>205.0</td>\n",
       "      <td>0.079112</td>\n",
       "      <td>0.114086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691135975539779</th>\n",
       "      <td>197.0</td>\n",
       "      <td>0.169778</td>\n",
       "      <td>0.153481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691135755698898</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.056407</td>\n",
       "      <td>0.199525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691136124845606</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.037570</td>\n",
       "      <td>0.123439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691135684367858</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.141867</td>\n",
       "      <td>0.135119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691135851792071</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.121208</td>\n",
       "      <td>0.042567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864691136021373176</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040951</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pre_pt_root_id     meanC      stdC\n",
       "864691136521831825           293.0  0.069778  0.143706\n",
       "864691135473091634           218.0  0.013846  0.114538\n",
       "864691136084747372           216.0  0.174026  0.161509\n",
       "864691135701034107           205.0  0.079112  0.114086\n",
       "864691135975539779           197.0  0.169778  0.153481\n",
       "...                            ...       ...       ...\n",
       "864691135755698898             4.0  0.056407  0.199525\n",
       "864691136124845606             3.0 -0.037570  0.123439\n",
       "864691135684367858             3.0  0.141867  0.135119\n",
       "864691135851792071             2.0  0.121208  0.042567\n",
       "864691136021373176             1.0  0.040951       NaN\n",
       "\n",
       "[452 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine these into a dataframe\n",
    "# label each of your series using X.name=\"series name\"\n",
    "# before combining them into a dataframe\n",
    "df = pd.DataFrame([counts, avgC, stdC]).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe366b2-5842-44b0-b995-110869ab64ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pre_pt_root_id', 'post_pt_root_id', 'n_syn', 'sum_size', 'pre_nuc_id',\n",
       "       'post_nuc_id', 'pre_pt_position', 'post_pt_position', 'index_pre',\n",
       "       'scan_session_pre', 'scan_idx_pre', 'field_pre', 'unit_id_pre',\n",
       "       'score_pre', 'residual_pre', 'index_post', 'scan_session_post',\n",
       "       'scan_idx_post', 'field_post', 'unit_id_post', 'score_post',\n",
       "       'residual_post', 'C', 'cell_type_pre', 'cell_type_post'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_syn_dfm3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f2d1270-6dba-4020-8744-39faf4701a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L2a', 'L2b', 'L3a', 'L3b', 'L3c', 'L4a', 'L4b', 'L4c', 'L5ET',\n",
       "       'L5NP', 'L5a', 'L5b', 'L6a'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_syn_dfm3[['cell_type_pre']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
